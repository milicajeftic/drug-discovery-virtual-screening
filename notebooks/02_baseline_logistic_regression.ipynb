{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd0e9fc",
   "metadata": {},
   "source": [
    "# 02 – Baseline Model: Logistic Regression\n",
    "\n",
    "**Project:** Predictive Modeling for Drug Discovery via Virtual Screening  \n",
    "**Student:** Milica Jeftić (ID: 89211255)  \n",
    "**Date:** January 2026  \n",
    "**Dataset:** Kaggle – Drug Discovery Virtual Screening Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Goal of This Notebook\n",
    "\n",
    "This notebook establishes a **baseline logistic regression model** as a reference point for all subsequent models.\n",
    "The objectives are:\n",
    "\n",
    "1. **Load Preprocessed Data** – Import train/validation/test sets from notebook 01\n",
    "2. **Train Baseline Model** – Fit logistic regression on training data\n",
    "3. **Validate Performance** – Evaluate on validation set with comprehensive metrics\n",
    "4. **Test Evaluation** – Final unbiased evaluation on held-out test set\n",
    "5. **Baseline Summary** – Document strengths, limitations, and reference point\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outputs\n",
    "\n",
    "- Baseline model performance metrics (accuracy, precision, recall, F1, ROC-AUC)\n",
    "- Confusion matrix and ROC curve visualizations\n",
    "- Saved baseline model (joblib)\n",
    "- Baseline report (saved to `results/metrics/`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d6bec",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "This section sets up the computational environment, imports required libraries, and defines global configuration used throughout the notebook to ensure reproducibility and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65e61f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Environment initialized successfully\n",
      "============================================================\n",
      "Python       : 3.10.19\n",
      "Numpy        : 2.2.5\n",
      "Pandas       : 2.3.3\n",
      "Scikit-learn : 1.7.2\n",
      "------------------------------------------------------------\n",
      "Project root : c:\\Users\\KORISNIK\\Documents\\drug-discovery-virtual-screening\n",
      "Data dir     : c:\\Users\\KORISNIK\\Documents\\drug-discovery-virtual-screening\\data\\processed\n",
      "Models dir   : c:\\Users\\KORISNIK\\Documents\\drug-discovery-virtual-screening\\models\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Environment & Configuration\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility & Warnings\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# Pandas display options\n",
    "# ----------------------------\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "\n",
    "# ----------------------------\n",
    "# Visualization defaults\n",
    "# ----------------------------\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "rcParams[\"figure.figsize\"] = (12, 6)\n",
    "rcParams[\"font.size\"] = 12\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ----------------------------\n",
    "# Project paths\n",
    "# ----------------------------\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "DATA_PROCESSED_PATH = os.path.join(PROJECT_ROOT, \"data\", \"processed\")\n",
    "MODELS_PATH = os.path.join(PROJECT_ROOT, \"models\")\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, \"results\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Environment initialized successfully\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python       : {sys.version.split()[0]}\")\n",
    "print(f\"Numpy        : {np.__version__}\")\n",
    "print(f\"Pandas       : {pd.__version__}\")\n",
    "print(f\"Scikit-learn : {__import__('sklearn').__version__}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Project root : {PROJECT_ROOT}\")\n",
    "print(f\"Data dir     : {DATA_PROCESSED_PATH}\")\n",
    "print(f\"Models dir   : {MODELS_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc26fd",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "We load the processed and scaled datasets from notebook 01.\n",
    "These are ready for modeling—no further preprocessing is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3596b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING PREPROCESSED DATA\n",
      "============================================================\n",
      "\n",
      "✓ Data loaded successfully\n",
      "\n",
      "Training set:\n",
      "  X_train: (1278, 14)\n",
      "  y_train: (1278,)\n",
      "\n",
      "Validation set:\n",
      "  X_val:   (274, 14)\n",
      "  y_val:   (274,)\n",
      "\n",
      "Test set:\n",
      "  X_test:  (274, 14)\n",
      "  y_test:  (274,)\n",
      "\n",
      "Class proportions (training set):\n",
      "active\n",
      "0   0.6956\n",
      "1   0.3044\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class proportions (validation set):\n",
      "active\n",
      "0   0.6934\n",
      "1   0.3066\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class proportions (test set):\n",
      "active\n",
      "0   0.6971\n",
      "1   0.3029\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "✓ StandardScaler loaded from c:\\Users\\KORISNIK\\Documents\\drug-discovery-virtual-screening\\models\\standard_scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load feature matrices\n",
    "X_train = pd.read_csv(os.path.join(DATA_PROCESSED_PATH, \"X_train.csv\"))\n",
    "X_val = pd.read_csv(os.path.join(DATA_PROCESSED_PATH, \"X_val.csv\"))\n",
    "X_test = pd.read_csv(os.path.join(DATA_PROCESSED_PATH, \"X_test.csv\"))\n",
    "\n",
    "# Load target vectors\n",
    "y_train = pd.read_csv(os.path.join(DATA_PROCESSED_PATH, \"y_train.csv\")).squeeze(\"columns\")\n",
    "y_val = pd.read_csv(os.path.join(DATA_PROCESSED_PATH, \"y_val.csv\")).squeeze(\"columns\")\n",
    "y_test = pd.read_csv(os.path.join(DATA_PROCESSED_PATH, \"y_test.csv\")).squeeze(\"columns\")\n",
    "\n",
    "# Sanity checks\n",
    "assert len(X_train) == len(y_train), \"Train X/y length mismatch\"\n",
    "assert len(X_val) == len(y_val), \"Val X/y length mismatch\"\n",
    "assert len(X_test) == len(y_test), \"Test X/y length mismatch\"\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  y_val:   {y_val.shape}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  X_test:  {X_test.shape}\")\n",
    "print(f\"  y_test:  {y_test.shape}\")\n",
    "\n",
    "# Verify class proportions across all sets\n",
    "print(f\"\\nClass proportions (training set):\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\nClass proportions (validation set):\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\nClass proportions (test set):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Load scaler for pipeline continuity\n",
    "scaler_path = os.path.join(MODELS_PATH, \"standard_scaler.joblib\")\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(f\"\\n✓ StandardScaler loaded from {scaler_path}\")\n",
    "else:\n",
    "    print(f\"\\n Warning: Scaler not found at {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16341af7",
   "metadata": {},
   "source": [
    "## 3. Baseline Model: Logistic Regression\n",
    "\n",
    "In this section, I initialize a Logistic Regression classifier to serve as an interpretable baseline.\n",
    "This model provides a linear decision boundary and will be used as a reference point when comparing\n",
    "more complex models later in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43124d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIALIZING BASELINE MODEL\n",
      "============================================================\n",
      "\n",
      "✓ Baseline model initialized\n",
      "\n",
      "Model parameters:\n",
      "  Algorithm       : Logistic Regression\n",
      "  Max iterations  : 1000\n",
      "  Class weight    : balanced\n",
      "  Solver          : lbfgs\n",
      "  Random state    : 42\n",
      "\n",
      "Rationale:\n",
      "  • Linear baseline for interpretability\n",
      "  • Balanced weights to handle ~70/30 class imbalance\n",
      "  • Provides reference point for complex models\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INITIALIZING BASELINE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize Logistic Regression with balanced weights\n",
    "# to account for the class imbalance in the dataset\n",
    "baseline_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Baseline model initialized\")\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Algorithm       : Logistic Regression\")\n",
    "print(f\"  Max iterations  : {baseline_model.max_iter}\")\n",
    "print(f\"  Class weight    : {baseline_model.class_weight}\")\n",
    "print(f\"  Solver          : {baseline_model.solver}\")\n",
    "print(f\"  Random state    : {baseline_model.random_state}\")\n",
    "print(f\"\\nRationale:\")\n",
    "print(f\"  • Linear baseline for interpretability\")\n",
    "print(f\"  • Balanced weights to handle ~70/30 class imbalance\")\n",
    "print(f\"  • Provides reference point for complex models\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383e9a6",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "This section trains the baseline model on the training dataset only.\n",
    "No hyperparameter tuning or evaluation occurs here, only fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit model on training data only\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n✓ Model training completed\")\n",
    "print(f\"\\nTraining time: {training_time:.4f} seconds\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Training features: {X_train.shape[1]}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
